**News Classification using BERT**

Overview

This project demonstrates a News Classification Model using BERT (Bidirectional Encoder Representations from Transformers). The primary objective is to accurately classify news articles into predefined categories, leveraging BERT's NLP capabilities for robust text classification. The project explores the effectiveness of BERT for categorizing news compared to traditional models like LSTM.
